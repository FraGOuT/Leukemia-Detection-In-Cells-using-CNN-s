{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='ALL_IDB2/ALL_IDB2/img'\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (width,height ), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = img.astype(np.float32)\n",
    "    return img\n",
    "\n",
    "def label_img(image_path):\n",
    "    if image_path[28] == \"0\":\n",
    "        return [1, 0]\n",
    "    else:\n",
    "        return [0, 1]\n",
    "\n",
    "def save_image(image, filename):\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    image = image.astype(np.uint8)\n",
    "    img = Image.fromarray(image)\n",
    "    print(img.show())\n",
    "    \n",
    "def load_image2(image_path):\n",
    "    #print(image_path)\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((width, height))\n",
    "    img\n",
    "    image_arr = np.asarray(img, dtype='float32')\n",
    "    image_arr = np.expand_dims(image_arr, axis=0)\n",
    "    \n",
    "    return img \n",
    "#xx = load_image('ALL_IDB2/ALL_IDB2/img\\\\Im004_1.tif')\n",
    "#save_image(xx,'yt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Collect all the image addresses\n",
    "image_address = []\n",
    "image_address.extend(glob.glob(data_path+'/*.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prep training data.\n",
    "def create_training_data():\n",
    "    data = []\n",
    "    for addr in image_address:\n",
    "        label = label_img(addr)\n",
    "        image = load_image(addr)\n",
    "        data.append([np.array(image),np.array(label)])\n",
    "    shuffle(data)\n",
    "    \n",
    "    training_data = data[:210]\n",
    "    testing_data = data[210:]\n",
    "    \n",
    "    print(len(training_data))\n",
    "    print(len(testing_data))\n",
    "    \n",
    "    return training_data,testing_data\n",
    "\n",
    "#get data and labels from the created data.\n",
    "def create_req(x):\n",
    "    x_data = []\n",
    "    x_label = []\n",
    "    for i in range(len(x)):\n",
    "        x_data.append(x[i][0])\n",
    "        x_label.append(x[i][1])\n",
    "    \n",
    "    return x_data,x_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "training_data, testing_data = create_training_data()\n",
    "\n",
    "train_x, train_y = create_req(training_data)\n",
    "test_x, test_y = create_req(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')\n",
    "\n",
    "out_num = 2\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def conv_net(x):\n",
    "    x = tf.reshape(x, [-1, 256, 256, 1])\n",
    "    W_conv1 = weight_variable([32, 32, 1, 16])\n",
    "    W_conv2 = weight_variable([32, 32, 16, 32])\n",
    "    #W_conv3 = weight_variable([32, 32, 32, 32])\n",
    "    W_fc1 = weight_variable([16*16*32, 512])\n",
    "    W_out = weight_variable([512, out_num])\n",
    "    \n",
    "    b_conv1 = bias_variable([16])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    #b_conv3 = bias_variable([32])\n",
    "    b_fc1 = bias_variable([512])\n",
    "    b_out = bias_variable([out_num])\n",
    "    \n",
    "    conv1 = maxpool( tf.nn.relu(conv2d(x,W_conv1)+b_conv1) )\n",
    "    conv2 = maxpool( tf.nn.relu(conv2d(conv1,W_conv2)+b_conv2) )\n",
    "    #conv3 = maxpool( tf.nn.relu(conv2d(conv2,W_conv3)+b_conv3) )\n",
    "    \n",
    "    fc1 = tf.reshape(conv2,[-1, 16*16*32 ])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1,W_fc1)+b_fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.8)\n",
    "    \n",
    "    output = (tf.matmul(fc1,W_out))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(x):\n",
    "    \n",
    "    predictions = conv_net(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = predictions,labels = y))\n",
    "    #learning_rate = \n",
    "    optimizer = tf.train.AdamOptimizer(0.00001).minimize(cost)\n",
    "    \n",
    "    hm_epoches = 80\n",
    "    batch_size = 5\n",
    "    \n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "    \n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Started Training ----\")\n",
    "        \n",
    "    for epoch in range(hm_epoches):\n",
    "        epoch_loss = 0\n",
    "        i = 0\n",
    "        \n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        while i< len(train_x):\n",
    "            start = i;\n",
    "            end = i+batch_size\n",
    "            batch_x = train_x[start:end]\n",
    "            batch_y = train_y[start:end]\n",
    "                \n",
    "            _,c = sess.run([optimizer, cost], feed_dict={x:batch_x , y:batch_y})\n",
    "            epoch_loss+=c\n",
    "            print('Batch',i,'Epoch',epoch,'/',hm_epoches,'loss:',c)\n",
    "            i=end\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(predictions,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        acc_test = 0\n",
    "        if epoch%5 == 0:\n",
    "            acc = accuracy.eval(session=sess, feed_dict={x:batch_x, y:batch_y})\n",
    "            #acc_test = accuracy.eval(session=sess, feed_dict={x:test_x, y:test_y})\n",
    "            print('Epoch',epoch,'Loss:',epoch_loss,'Train Accuracy:',acc,'Test : ',acc_test)\n",
    "        else:\n",
    "            print('Epoch',epoch,'Loss:',epoch_loss)\n",
    "        #acc1 = accuracy.eval({x:train_x, y:train_y})\n",
    "        \n",
    "        #acc = accuracy.eval(session=sess, feed_dict={x:test_x, y:test_y})\n",
    "        loss_arr.append(epoch_loss)\n",
    "        #acc_arr.append(acc_test)\n",
    "        \n",
    "        #print('Epoch',epoch,'Loss:',epoch_loss,' Test Accuracy:',acc)\n",
    "        \n",
    "    return loss_arr, sess\n",
    "#Accuracy: 0.666667   learning rate=0.0005   epoches = 10\n",
    "#Accuracy: 0.833333   learning rate=0.0002   epoches = 20\n",
    "#Accuracy: 0.833333   learning rate=0.0002   epoches = 20    Dropout = True 0.8 --WASTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "acc = []\n",
    "\n",
    "loss, sess = train_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training ----\n"
     ]
    }
   ],
   "source": [
    "predictions = conv_net(x)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = predictions,labels = y))\n",
    "\n",
    "#learning_rate = \n",
    "optimizer = tf.train.AdamOptimizer(0.0001).minimize(cost)    \n",
    "hm_epoches = 50\n",
    "batch_size = 5\n",
    "    \n",
    "loss_arr = []\n",
    "acc_arr = []\n",
    "    \n",
    "print(\"Started Training ----\")\n",
    "   \n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "for epoch in range(hm_epoches):\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "        \n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "        \n",
    "    while i< len(train_x):\n",
    "        start = i;\n",
    "        end = i+batch_size\n",
    "        batch_x = train_x[start:end]\n",
    "        batch_y = train_y[start:end]\n",
    "            \n",
    "        _,c = sess.run([optimizer, cost], feed_dict={x:batch_x , y:batch_y})\n",
    "        epoch_loss+=c\n",
    "        print('Batch',i,'Epoch',epoch,'/',hm_epoches,'loss:',c)\n",
    "        i=end\n",
    "        \n",
    "    correct = tf.equal(tf.argmax(predictions,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "    if epoch%5 == 0:\n",
    "        acc = accuracy.eval(session=sess, feed_dict={x:batch_x, y:batch_y})\n",
    "        print('Epoch',epoch,'Loss:',epoch_loss,'Train Accuracy:',acc)\n",
    "    else:\n",
    "        print('Epoch',epoch,'Loss:',epoch_loss)\n",
    "        \n",
    "    #acc1 = accuracy.eval({x:train_x, y:train_y})     \n",
    "    #acc = accuracy.eval(session=sess, feed_dict={x:test_x, y:test_y})\n",
    "    loss_arr.append(epoch_loss)\n",
    "    #acc_arr.append(acc)\n",
    "    #print('Epoch',epoch,'Loss:',epoch_loss,' Test Accuracy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.figure(1)\n",
    "x = range(len(loss))\n",
    "#print(x)\n",
    "plt.plot(x, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = conv_net(x)\n",
    "correct = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "test_er = accuracy.eval(session=sess, feed_dict={x:test_x, y:test_y})\n",
    "print(test_er)\n",
    "#xx = range(len(loss))\n",
    "#plt.plot(xx,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range(len(acc)):\n",
    "#    acc[i] = acc[i]*100\n",
    "#print(acc)\n",
    "plt.plot(x, acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(xx, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
